{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b597a4-ed8b-4011-8f95-79faf9168f50",
   "metadata": {},
   "source": [
    "# NLP Lab 2 - Nick von Bulow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8019427-e5a6-4164-9986-a6a4d87fc264",
   "metadata": {},
   "source": [
    "## Removing HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a90b50-df95-4c3d-b14f-e455e547d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "data = requests.get('http://www.gutenberg.org/cache/epub/8001/pg8001.html')\n",
    "content = data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f35da5-cb32-45c7-b50d-5d848232467f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' Do not change or edit the\\r\\nheader without written permission.</p>\\r\\n\\r\\n<p id=\"id00003\">Please read the \"legal small print,\" and other information about the\\r\\neBook and Project Gutenberg at the bottom of this file.  Included is\\r\\nimportant information about your specific rights and restrictions in\\r\\nhow the file may be used.  You can also find out about how to make a\\r\\ndonation to Project Gutenberg, and how to get involved.</p>\\r\\n\\r\\n<p id=\"id00004\" style=\"margin-top: 2em\">**Welcome To The World of Free '\n"
     ]
    }
   ],
   "source": [
    "# The HTML tags make it hard to read\n",
    "print(content[5000:5500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a673800f-7ebc-4a98-a44c-1207311c93d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d of schedule]\n",
      "[This file was first posted on June 7, 2003]\n",
      "Edition: 10\n",
      "Language: English\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK, THE BIBLE, KING JAMES, BOOK 1***\n",
      "This eBook was produced by David Widger\n",
      "with the help of Derek Andrew's text from January 1992\n",
      "and the work of Bryan Taylor in November 2002.\n",
      "Book 01        Genesis\n",
      "01:001:001 In the beginning God created the heaven and the earth.\n",
      "01:001:002 And the earth was without form, and void; and darkness was\n",
      "           upon the face of the deep. And the Spirit of God moved upon\n",
      "           the face of the waters.\n",
      "01:001:003 And God said, Let there be light: and there was light.\n",
      "01:001:004 And God saw the light, that it was good: and God divided the\n",
      "           light from the darkness.\n",
      "01:001:005 And God called the light Day, and the darkness he called\n",
      "           Night. And the evening and the morning were the first day\n"
     ]
    }
   ],
   "source": [
    "# Solution: use a library to strip these html tags\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    [s.extract() for s in soup(['iframe', 'script'])]\n",
    "    stripped_txt = soup.get_text()\n",
    "    stripped_txt = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_txt)\n",
    "    return stripped_txt\n",
    "\n",
    "clean_content = strip_html_tags(content)\n",
    "print(clean_content[1163:2045])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03eded1-98e1-4693-b4ee-255decd0f414",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c5d1a5-4ef4-42ed-a472-57deb62d0d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# Load the text\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = (\"US unveils world's most powerful supercomputer, beats China. The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight. With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second. Summit has 4,608 servers, which reportedly take up the size of two tennis courts.\")\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290ff249-35a2-4115-bab3-d57e9d18ac33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the length of the alice text\n",
    "len(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de58093b-b1f6-40ca-8204-0093d5018a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "alice[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f46eb8-5e92-4a45-91fd-12193093b77e",
   "metadata": {},
   "source": [
    "### Default Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9510c409-5a0e-45f5-8da3-e1e45e5549dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 4\n",
      "Sample text sentences :-\n",
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\"\n",
      " \"Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\"\n",
      " 'So she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.'\n",
      " \"There was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\"\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "print(np.array(sample_sentences))\n",
    "\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "print(np.array(alice_sentences[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c3cf2-3a45-41fa-88d3-63ef35ed8712",
   "metadata": {},
   "source": [
    "### Pretrained Sentence Tokenizer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a039d67-fa4f-483a-8787-d629f73c662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total characters in the corpus\n",
    "print(len(german_text))\n",
    "# First 100 characters\n",
    "print(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f52fff23-aaf9-465d-b511-a36f2dee8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text!\n",
    "german_sentences_def = default_st(text=german_text, language='german')\n",
    "\n",
    "# Load the pretrained German tokenizer model\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee7ef4f-e16a-4437-9be5-6406537745b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# Let's check out the type of our tokenizer\n",
    "print(type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecbbaa43-9e14-4dcd-a0e6-c1fd239b0540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Do the results of each tokenizer match?\n",
    "print(german_sentences_def == german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "051ba5b7-1616-41df-abd7-81a8b4cf56d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nWiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .'\n",
      " 'Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .'\n",
      " 'Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .'\n",
      " 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .'\n",
      " 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .']\n"
     ]
    }
   ],
   "source": [
    "# print the first five sentences of the German corpus\n",
    "print(np.array(german_sentences[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0b7ba-3e8d-4382-9052-0f501c7adb1a",
   "metadata": {},
   "source": [
    "### PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "674022c7-a024-473c-96ee-af928c72ffc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "print(np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b27f0d-5776-454e-b240-008ce2a2c11f",
   "metadata": {},
   "source": [
    "### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "432d8713-7f02-472c-90d5-152c9e36a5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"US unveils world's most powerful supercomputer, beats China.\"\n",
      " \"The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.\"\n",
      " 'With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.'\n",
      " 'Summit has 4,608 servers, which reportedly take up the size of two tennis courts.']\n"
     ]
    }
   ],
   "source": [
    "# define a regex string to identify sentence tokens\n",
    "SENTENCE_TOKEN_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(\n",
    "    pattern=SENTENCE_TOKEN_PATTERN,\n",
    "    gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "print(np.array(sample_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc48b667-606f-4c9c-9578-38d8e88ed9ec",
   "metadata": {},
   "source": [
    "## Word Tokenization\n",
    "\n",
    "We are exploring the following interfaces:\n",
    "- `word_tokenize`\n",
    "- `TreebankWordTokenizer`\n",
    "- `TokTokTokenizer`\n",
    "- `RegexpTokenizer`\n",
    "- `Inherited tokenizers from RegexpTokenizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b997fed-0643-46e0-84c7-7578c3b589a6",
   "metadata": {},
   "source": [
    "### Default Word Tokenizer (`nltk.word_tokenize`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65efcfb-398b-49d4-8b5c-2c0172380198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(sample_text)\n",
    "# Notice how this one doesn't properly handle apostrophes\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a301643-41a8-40dd-8545-c7aebc6144bd",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23ec400e-5ec8-4067-aaea-c6a19a6fe789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway',\n",
       "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0d444-8f72-40cd-b190-6e8acd186bd0",
   "metadata": {},
   "source": [
    "### TokTokTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8e30a08-3ea2-46c3-b286-0384ff8908e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
       "       'the', 'previous', 'record-holder', 'China', \"'\", 's', 'Sunway',\n",
       "       'TaihuLight.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "tokenizer = ToktokTokenizer()\n",
    "words = tokenizer.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73302298-4f57-4d82-a282-01601f32fb48",
   "metadata": {},
   "source": [
    "### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a658e4b0-5be9-4d45-bdf2-0fa45f64bfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', 's', 'most', 'powerful', 'supercomputer',\n",
       "       'beats', 'China', 'The', 'US', 'has', 'unveiled', 'the', 'world',\n",
       "       's', 'most', 'powerful', 'supercomputer', 'called', 'Summit',\n",
       "       'beating', 'the', 'previous', 'record', 'holder', 'China', 's',\n",
       "       'Sunway', 'TaihuLight', 'With', 'a', 'peak', 'performance', 'of',\n",
       "       '200', '000', 'trillion', 'calculations', 'per', 'second', 'it',\n",
       "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
       "       'which', 'is', 'capable', 'of', '93', '000', 'trillion',\n",
       "       'calculations', 'per', 'second', 'Summit', 'has', '4', '608',\n",
       "       'servers', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts'], dtype='<U13')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pattern that takes repeated word characters until it can't anymore\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2d9cc4d-3007-4dd9-99c4-8ffac2079989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
       "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
       "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
       "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
       "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
       "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
       "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
       "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
       "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared to the last one, this tokenizer identifies gaps between words instead of the words themselves\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "words = regex_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "335c3d21-5531-434d-b36c-22168ca1afc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 2), (3, 10), (11, 18), (19, 23), (24, 32), (33, 47), (48, 53), (54, 60), (61, 64), (65, 67), (68, 71), (72, 80), (81, 84), (85, 92), (93, 97), (98, 106), (107, 120), (121, 127), (128, 137), (138, 145), (146, 149), (150, 158), (159, 172), (173, 180), (181, 187), (188, 199), (200, 204), (205, 206), (207, 211), (212, 223), (224, 226), (227, 234), (235, 243), (244, 256), (257, 260), (261, 268), (269, 271), (272, 274), (275, 279), (280, 285), (286, 288), (289, 293), (294, 296), (297, 303), (304, 315), (316, 321), (322, 324), (325, 332), (333, 335), (336, 342), (343, 351), (352, 364), (365, 368), (369, 376), (377, 383), (384, 387), (388, 393), (394, 402), (403, 408), (409, 419), (420, 424), (425, 427), (428, 431), (432, 436), (437, 439), (440, 443), (444, 450), (451, 458)]\n",
      "['US' 'unveils' \"world's\" 'most' 'powerful' 'supercomputer,' 'beats'\n",
      " 'China.' 'The' 'US' 'has' 'unveiled' 'the' \"world's\" 'most' 'powerful'\n",
      " 'supercomputer' 'called' \"'Summit',\" 'beating' 'the' 'previous'\n",
      " 'record-holder' \"China's\" 'Sunway' 'TaihuLight.' 'With' 'a' 'peak'\n",
      " 'performance' 'of' '200,000' 'trillion' 'calculations' 'per' 'second,'\n",
      " 'it' 'is' 'over' 'twice' 'as' 'fast' 'as' 'Sunway' 'TaihuLight,' 'which'\n",
      " 'is' 'capable' 'of' '93,000' 'trillion' 'calculations' 'per' 'second.'\n",
      " 'Summit' 'has' '4,608' 'servers,' 'which' 'reportedly' 'take' 'up' 'the'\n",
      " 'size' 'of' 'two' 'tennis' 'courts.']\n"
     ]
    }
   ],
   "source": [
    "# let's obtain the token boundaries\n",
    "word_indicies = list(regex_wt.span_tokenize(sample_text))\n",
    "print(word_indicies)\n",
    "print(np.array([sample_text[start:end] for start, end in word_indicies]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fc124-8948-4630-8bbd-37416cf81395",
   "metadata": {},
   "source": [
    "### Inherited Tokenizers from RegexpTokenizer\n",
    "\n",
    "There are several classes derived from RegexpTokenizer with predefined patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae6a2f89-209e-4e22-b0bd-4f7cb7798852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'\", 's', 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', \"',\", 'beating', 'the',\n",
       "       'previous', 'record', '-', 'holder', 'China', \"'\", 's', 'Sunway',\n",
       "       'TaihuLight', '.', 'With', 'a', 'peak', 'performance', 'of', '200',\n",
       "       ',', '000', 'trillion', 'calculations', 'per', 'second', ',', 'it',\n",
       "       'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight',\n",
       "       ',', 'which', 'is', 'capable', 'of', '93', ',', '000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4', ',',\n",
       "       '608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the wordpunkt tokenizer separates out punctuation into its own tokens\n",
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87efca63-7683-4fe4-9526-df382b3ad1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', \"world's\", 'most', 'powerful', 'supercomputer,',\n",
       "       'beats', 'China.', 'The', 'US', 'has', 'unveiled', 'the',\n",
       "       \"world's\", 'most', 'powerful', 'supercomputer', 'called',\n",
       "       \"'Summit',\", 'beating', 'the', 'previous', 'record-holder',\n",
       "       \"China's\", 'Sunway', 'TaihuLight.', 'With', 'a', 'peak',\n",
       "       'performance', 'of', '200,000', 'trillion', 'calculations', 'per',\n",
       "       'second,', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as',\n",
       "       'Sunway', 'TaihuLight,', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second.', 'Summit', 'has',\n",
       "       '4,608', 'servers,', 'which', 'reportedly', 'take', 'up', 'the',\n",
       "       'size', 'of', 'two', 'tennis', 'courts.'], dtype='<U14')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizes sentences based on white space\n",
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sample_text)\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d929a29-3cd0-409c-a50e-d6d7913770d1",
   "metadata": {},
   "source": [
    "## Building Robust Tokenizers with NLTK and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3b60c1d-1e74-4220-98e3-079a47ae9cc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['US', 'unveils', 'world', \"'s\", 'most', 'powerful', 'supercomputer', ',', 'beats', 'China', '.']),\n",
       "       list(['The', 'US', 'has', 'unveiled', 'the', 'world', \"'s\", 'most', 'powerful', 'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the', 'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight', '.']),\n",
       "       list(['With', 'a', 'peak', 'performance', 'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',', 'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000', 'trillion', 'calculations', 'per', 'second', '.']),\n",
       "       list(['Summit', 'has', '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size', 'of', 'two', 'tennis', 'courts', '.'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Easiest way to develop an NLP pipeline that needs to tokenize is by leveraging state of the art libraries\n",
    "\n",
    "# nltk method of tokenizing text\n",
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "sents = tokenize_text(sample_text)\n",
    "np.array(sents, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52fd93b-5973-4824-862f-3b661abcc22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'Summit\", \"'\", ',', 'beating', 'the',\n",
       "       'previous', 'record-holder', 'China', \"'s\", 'Sunway', 'TaihuLight',\n",
       "       '.', 'With', 'a', 'peak', 'performance', 'of', '200,000',\n",
       "       'trillion', 'calculations', 'per', 'second', ',', 'it', 'is',\n",
       "       'over', 'twice', 'as', 'fast', 'as', 'Sunway', 'TaihuLight', ',',\n",
       "       'which', 'is', 'capable', 'of', '93,000', 'trillion',\n",
       "       'calculations', 'per', 'second', '.', 'Summit', 'has', '4,608',\n",
       "       'servers', ',', 'which', 'reportedly', 'take', 'up', 'the', 'size',\n",
       "       'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattening the list of tokenized sentences\n",
    "words = [word for sentence in sents for word in sentence]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5eeb5794-306d-4baf-b9c7-8c2b23856886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([US unveils world's most powerful supercomputer, beats China.,\n",
       "       The US has unveiled the world's most powerful supercomputer called 'Summit', beating the previous record-holder China's Sunway TaihuLight.,\n",
       "       With a peak performance of 200,000 trillion calculations per second, it is over twice as fast as Sunway TaihuLight, which is capable of 93,000 trillion calculations per second.,\n",
       "       Summit has 4,608 servers, which reportedly take up the size of two tennis courts.],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text_spacy = nlp(sample_text)\n",
    "\n",
    "sents = np.array(list(text_spacy.sents), dtype='object')\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "853e89ef-9efe-45ea-9e94-158024f79399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['US', 'unveils', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', ',', 'beats', 'China', '.', 'The', 'US', 'has',\n",
       "       'unveiled', 'the', 'world', \"'s\", 'most', 'powerful',\n",
       "       'supercomputer', 'called', \"'\", 'Summit', \"'\", ',', 'beating',\n",
       "       'the', 'previous', 'record', '-', 'holder', 'China', \"'s\",\n",
       "       'Sunway', 'TaihuLight', '.', 'With', 'a', 'peak', 'performance',\n",
       "       'of', '200,000', 'trillion', 'calculations', 'per', 'second', ',',\n",
       "       'it', 'is', 'over', 'twice', 'as', 'fast', 'as', 'Sunway',\n",
       "       'TaihuLight', ',', 'which', 'is', 'capable', 'of', '93,000',\n",
       "       'trillion', 'calculations', 'per', 'second', '.', 'Summit', 'has',\n",
       "       '4,608', 'servers', ',', 'which', 'reportedly', 'take', 'up',\n",
       "       'the', 'size', 'of', 'two', 'tennis', 'courts', '.'], dtype='<U13')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word.text for word in text_spacy]\n",
    "np.array(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa167f-7c7b-44a5-b296-efe2542dac9a",
   "metadata": {},
   "source": [
    "## Removing Accented Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65f2c2ef-a077-4b9d-b537-a5783e18758d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Some Accented text'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "remove_accented_chars('Sómě Áccěntěd těxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e7b50-5258-437f-86e4-fb5f3e4ee064",
   "metadata": {},
   "source": [
    "## Expanding Contractions\n",
    "\n",
    "Brute force method using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0884946f-d77c-4f7a-ae85-50e7fe62859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You all cannot expand contractions I would think'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "    \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", '', expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "expand_contractions(\"Y'all can't expand contractions I'd think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ececaf-2b48-4937-bdba-0f2db3391697",
   "metadata": {},
   "source": [
    "## Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9f289f8-0b28-4652-ba86-7edcc9b3e47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Well this was fun What do you think '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "remove_special_characters(\"Well this was fun! What do you think? 123#@!\",remove_digits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31049257-57aa-45a4-b97f-425750a450a7",
   "metadata": {},
   "source": [
    "## Text Correction\n",
    "\n",
    "### Correcting Repeating Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9a8ef1a-3f20-4a46-818d-95607a93cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Step: 4 Word: finaly\n",
      "Final word:  finaly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'finaly'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'finalllyyy'\n",
    "\n",
    "def remove_repeating_characters(word):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    step = 1\n",
    "    old_word = word\n",
    "    while True:\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        if new_word != old_word:\n",
    "            print(f'Step: {step} Word: {new_word}')\n",
    "            step += 1\n",
    "            old_word = new_word\n",
    "            continue\n",
    "        else:\n",
    "            print('Final word: ', new_word)\n",
    "            break\n",
    "    return new_word\n",
    "\n",
    "remove_repeating_characters(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e4d7e2f-43b0-4c4c-a755-51e1317eb2a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My school is really amazing'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternate method that stops the loop when the word is semantically correct\n",
    "from nltk.corpus import wordnet\n",
    "old_word = 'finalllyyy'\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    \n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "    \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "correct_tokens = remove_repeated_characters(nltk.word_tokenize(sample_sentence))\n",
    "' '.join(correct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1144aa-d140-48cf-bf95-924f1758719a",
   "metadata": {},
   "source": [
    "### Correcting Spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0e6fbdd4-8882-4f22-aad3-513738e7e255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 80030),\n",
       " ('of', 40025),\n",
       " ('and', 38313),\n",
       " ('to', 28766),\n",
       " ('in', 22050),\n",
       " ('a', 21155),\n",
       " ('that', 12512),\n",
       " ('he', 12401),\n",
       " ('was', 11410),\n",
       " ('it', 10681)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "def tokens(text):\n",
    "    return re.findall('[a-z]+', text.lower())\n",
    "\n",
    "WORDS = tokens(open('big.txt').read())\n",
    "WORD_COUNTS = collections.Counter(WORDS)\n",
    "# top 10 words contained in the text file\n",
    "WORD_COUNTS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7157c38f-2fbe-49f1-bd2e-00be5e84fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits(word, count=1):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    def splits(word):\n",
    "        return [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    \n",
    "    def edit_word(word):\n",
    "        pairs = splits(word)\n",
    "        deletes = [a+b[1:] for (a, b) in pairs if b]\n",
    "        transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "        replaces = [a+c+b[1:] for (a, b) in pairs for c in alphabet if b]\n",
    "        inserts = [a+c+b for (a, b) in pairs for c in alphabet]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "    \n",
    "    words = [word]\n",
    "    # perform `count` edits\n",
    "    for i in range(count):\n",
    "        new_words = [nw for w in words for nw in edit_word(w)]\n",
    "        # deduplicate the list and keep editing\n",
    "        words = set(new_words)\n",
    "    \n",
    "    return words\n",
    "\n",
    "def known(words):\n",
    "    return { w for w in words if w in WORD_COUNTS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1eb82bab-36c1-414f-bcef-64d12f06d2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finall']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'finall'\n",
    "edits(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "160910bd-f870-4685-ba2c-5e1a7d727a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits(word, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd320e5f-e569-475b-9f9a-eb6b2fa9a2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final', 'finally'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits(word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "afaddd99-2fc1-4f58-ade2-d2b7a38964bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fall', 'fill', 'final', 'finally', 'finely'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known(edits(word, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "349fc303-f234-4e24-b9ed-9990a22b271f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct(word):\n",
    "    candidates = (known(edits(word, 2)) or [word])\n",
    "    return max(candidates, key=WORD_COUNTS.get)\n",
    "\n",
    "correct('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "365ce711-5141-41a8-b238-0c0036d6c1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def correct_match(match):\n",
    "    word = match.group()\n",
    "    def case_of(text):\n",
    "        return (str.upper if text.isupper() else\n",
    "                str.lower if text.islower() else\n",
    "                str.title if text.istitle() else\n",
    "                str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def correct_text_generic(text):\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "correct_text_generic('fianlly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bd60044b-a8ef-464a-b19a-7bd0c972b70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FINALLY'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text_generic('FIANLLY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "effd4da5-9c8d-41bc-b206-51dfb6d3fa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finally'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now for a library that does this for us\n",
    "\n",
    "from textblob import Word\n",
    "w = Word('fianlly')\n",
    "w.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2454386e-8c0b-41ee-8c58-dde5c48376f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finally', 1.0)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e824e578-2e7a-4307-8906-76e4645bdd0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flat', 0.85), ('float', 0.15)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('flaot')\n",
    "w.spellcheck()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2876fc0-d016-4079-964a-d5d50b5a2ee5",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a67d16d1-4044-4383-833d-6ee905373a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.stem.porter.PorterStemmer'> jump jump jump\n",
      "<class 'nltk.stem.lancaster.LancasterStemmer'> jump jump jump\n",
      "<class 'nltk.stem.regexp.RegexpStemmer'> jump jump jump\n",
      "<class 'nltk.stem.snowball.SnowballStemmer'> jump jump jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, SnowballStemmer\n",
    "\n",
    "stemmers = [PorterStemmer(), \n",
    "            LancasterStemmer(), \n",
    "            RegexpStemmer('ing$|s$|ed$', min=4), \n",
    "            SnowballStemmer(language='english')]\n",
    "\n",
    "for st in stemmers:\n",
    "    print(type(st), st.stem('jumping'), st.stem('jumps'), st.stem('jumping'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e937a506-6fa9-46b4-9c50-ad29d7a52948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my system keep crash hi crash yesterday, our crash daili'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simple_stemmer(text, st=PorterStemmer()):\n",
    "    text = ' '.join([st.stem(word) for word in text.split()])\n",
    "    return text\n",
    "\n",
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63bfea-9564-4962-93b5-befbdf224fd7",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ae2d7020-b78d-48ac-a44b-ccea373be33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n",
      "run\n",
      "eat\n",
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatization using WordNet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# nouns\n",
    "print(wnl.lemmatize('cars', 'n'))\n",
    "print(wnl.lemmatize('men', 'n'))\n",
    "\n",
    "# verbs\n",
    "print(wnl.lemmatize('running', 'v'))\n",
    "print(wnl.lemmatize('ate', 'v'))\n",
    "\n",
    "# adjectives\n",
    "print(wnl.lemmatize('saddest', 'a'))\n",
    "print(wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f93ca138-45f4-488b-8b1a-ecddba3dafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my system keep crash his crashed yesterday , ours crash daily\n",
      "my system keep crash ! his crash yesterday , ours crash daily\n"
     ]
    }
   ],
   "source": [
    "# lemmatization using spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text1 = \"My system keeps crashing his crashed yesterday, ours crashes daily\"\n",
    "text2 = \"My system keeps crashing! his crashed yesterday, ours crashes daily\"\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "print(lemmatize_text(text1))\n",
    "print(lemmatize_text(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c7c0e8-2796-41e1-972c-cab982713feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
